{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 前言"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 Machine Learning 中，我們經常需要定義(Define)、儲存(Save)或是回復(Restore)模型。模型其實就是一個 Function，且 Function 裡頭的參數能夠在 Training 時被更新。\n",
    "\n",
    "本篇在於了解 Keras 底下的 TensorFlow 是如何運作的！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 了解 Model 與 Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "大部分的 Model 都是由許多 Layer 所組成。我們可以將每一層 Layer 視為一組數學運算（例如：矩陣運算），Layer 中也包含許多可以訓練的參數。Keras 中提供許多 High-Level 的 API 來實作 Layer 與 Model，其實他們的底層是 TensorFlow Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a simple tensorflow module\n",
    "class SimpleModule(tf.Module):\n",
    "    def __init__(self, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.a_variable = tf.Variable(5.0, name=\"train_me\")\n",
    "        self.a_non_trainable_variable = tf.Variable(5.0, trainable=False, name=\"do_not_train_me\")\n",
    "    def __call__(self, x):\n",
    "        return self.a_variable * x + self.a_non_trainable_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=30.0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_module = SimpleModule(name=\"simple\")\n",
    "simple_module(tf.constant(5.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不管是 Module 還是基於它的 Layer 和 Model，他們都是 Python Object。因此，他們都會有 Internal State (Property)，以及使用那些 State 的 Method。\n",
    "\n",
    "執得注意的是，當一個 SimpleModule Class 繼承 TensorFlow Module 後，SimpleModule Object 中的 Property 如果是 TensorFlow Variable 或是 TensorFlow Module 都會自動被「收集」。因此，能夠輕易的儲存、載入、更新這些 Variable。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Variable 'train_me:0' shape=() dtype=float32, numpy=5.0>,)\n",
      "(<tf.Variable 'do_not_train_me:0' shape=() dtype=float32, numpy=5.0>, <tf.Variable 'train_me:0' shape=() dtype=float32, numpy=5.0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-27 20:28:35.755971: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    }
   ],
   "source": [
    "# all trainable variable\n",
    "print(simple_module.trainable_variables)\n",
    "\n",
    "# all variable\n",
    "print(simple_module.variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 建立一個 Two Layer (Linear Layer) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 先定義 Dense Layer\n",
    "class Dense(tf.Module):\n",
    "    def __init__(self, in_feature, out_feature, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.w = tf.Variable(tf.random.normal([in_feature, out_feature]), name='w')\n",
    "        self.b = tf.Variable(tf.zeros([out_feature]), name='b')\n",
    "    def __call__(self, x):\n",
    "        y = tf.matmul(x, self.w) + self.b\n",
    "        return tf.nn.relu(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 再定義一個 Model\n",
    "class SequentialModel(tf.Module):\n",
    "    def __init__(self, name=None):\n",
    "        super().__init__(name=name)\n",
    "        \n",
    "        self.dense_1 = Dense(in_feature=3, out_feature=3)\n",
    "        self.dense_2 = Dense(in_feature=3, out_feature=2)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = self.dense_1(x)\n",
    "        return self.dense_2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.       5.364657]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "my_model = SequentialModel()\n",
    "print(my_model(tf.constant([[2.0, 2.0, 2.0]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因為 SequentialModel Object 是一個 TensorFlow Module，因此他會自動「收集」TensorFlow Module or TensorFlow Variable Property。使得我們可以簡單的儲存、使用一個 Model 中的所有 TensorFlow Module 與 TensorFlow Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<__main__.Dense at 0x12645cc10>, <__main__.Dense at 0x105f43700>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model.submodules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'b:0' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>,\n",
       " <tf.Variable 'w:0' shape=(3, 3) dtype=float32, numpy=\n",
       " array([[ 0.2657629 , -0.7628884 ,  1.3672371 ],\n",
       "        [-0.53525907,  0.91181624,  0.33850807],\n",
       "        [-0.59865075, -1.0534652 , -0.21244623]], dtype=float32)>,\n",
       " <tf.Variable 'b:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)>,\n",
       " <tf.Variable 'w:0' shape=(3, 2) dtype=float32, numpy=\n",
       " array([[-2.2294002, -0.7171399],\n",
       "        [ 0.941205 , -1.5262581],\n",
       "        [-0.8303009,  1.7962433]], dtype=float32)>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model.variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果使用過 tf.keras.layers.Dense 就可以發現，在建立 Dense 時，可以不用指定 Input Size。為了做到這樣，我們可以等到有東西傳入 Dense 時，再建立 Dense 裡面的 w 與 b。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlexibleDense(tf.Module):\n",
    "    # 不需要指定 in_feature\n",
    "    def __init__(self, out_features, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.is_built = False\n",
    "        self.out_features = out_features\n",
    "\n",
    "    def __call__(self, x):\n",
    "        if not self.is_built:\n",
    "            self.w = tf.Variable(tf.random.normal([x.shape[-1], self.out_features]), name='w')\n",
    "            self.b = tf.zeros([self.out_features], name='b')\n",
    "            self.is_built = True\n",
    "        y = tf.matmul(x, self.w) + self.b\n",
    "        return tf.nn.relu(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySequentialModel(tf.Module):\n",
    "    def __init__(self, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.dense_1 = FlexibleDense(out_features=3)\n",
    "        self.dense_2 = FlexibleDense(out_features=2)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        x = self.dense_1(x)\n",
    "        return self.dense_2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[5.1857915 3.1063278]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "my_model = MySequentialModel()\n",
    "print(my_model(tf.constant([[2.0, 2.0, 2.0]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 儲存 Model Weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通常我們說儲存一個 TensorFlow Model，經常指的是 (1) 將 TensorFlow Module 儲存為 Checkpoint 或是 (2) 儲存為SavedModel。\n",
    "\n",
    "Checkpoint 僅有紀錄 Module 與 Submodule 中的所有 TensorFlow Variable 的數值。當我們在使用 Checkpoint 時，Source Code 中還必須要有 Model 的結構，並載入這個 Chekcpoint，這個 Checkpoint 才有用。\n",
    "\n",
    "SavedModel 則會將 Model(TensorFlow Module) 中的所有運算流程都存下來，當然也包含 Checkpoint。當我們在使用 SavedModel 時，其實不用管 Source Code 中如何定義這個 Model。因此，SavedModel 很適合作為 Deployment 的工具。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'checkpoints/my_checkpoint'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chkp_path = 'checkpoints/my_checkpoint'\n",
    "checkpoint = tf.train.Checkpoint(model=my_model)\n",
    "checkpoint.write(chkp_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一份 Checkpoint 中會包含兩個檔案：Data 與 Metadata。Data 就是所有 TensorFlow Variable 的數值；Metadata 則是紀錄總共有多少個 Checkpoint。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_CHECKPOINTABLE_OBJECT_GRAPH', []),\n",
       " ('model/dense_1/w/.ATTRIBUTES/VARIABLE_VALUE', [3, 3]),\n",
       " ('model/dense_2/w/.ATTRIBUTES/VARIABLE_VALUE', [3, 2])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 看看一份 checkpoint 中記錄了什麼 variable\n",
    "tf.train.list_variables(chkp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x1264e2e20>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 將 checkpoint restore 回去 model\n",
    "my_model = MySequentialModel()\n",
    "new_checkpoint = tf.train.Checkpoint(model=my_model)\n",
    "new_checkpoint.restore(chkp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[5.1857915, 3.1063278]], dtype=float32)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model(tf.constant([[2.0, 2.0, 2.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果要將模型進行部署與分享，比較好的儲存模型的方法是透過 SavedModel。\n",
    "\n",
    "`tf.saved_model.save()`\n",
    "會建立一個資料夾，裡頭的 variables 資料夾代表 Model 的 Checkpoint (Model 中所有 Variable 的數值)；pb 檔案則是一個 protocol buffer 用來描述 Model 的 TensorFlow Graph。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_saved_model/assets\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(my_model, \"my_saved_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "直接透過 `tf.saved_model.load()` 即可將剛剛儲存的模型載入！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = tf.saved_model.load('my_saved_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type: <class 'tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 但是新載入的模型是 TensorFlow User Object 而不是原先的 MySequentialModel Object\n",
    "print(f'type: {type(new_model)}')\n",
    "isinstance(new_model, MySequentialModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 瞭解 Keras Model 與 Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras 中的 Layer 與 Model (High Level API) 最底層皆是繼承自 TensorFlow Module。舉例來說，`tf.keras.layers.Layer` 是 Keras 所有 layers 的 base class，其就繼承自 `tf.Module` (TensorFlow Module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們可以簡單的建立一個 Keras Layer，只需要將原來繼承自 `tf.Module` 改為 `tf.keras.layers.Layer`，並將 `__call__` 更改為 `call`。因為 Keras Layer 會在 `__call__` 中實作一些功能再呼叫 `call`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDense(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, in_features, out_features, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.w = tf.Variable(tf.random.normal([in_features, out_features]), name='w')\n",
    "        self.b = tf.Variable(tf.zeros([out_features]), name='b')\n",
    "\n",
    "    def call(self, x):\n",
    "        y = tf.matmul(x, self.w) + self.b\n",
    "        return tf.nn.relu(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_layer = MyDense(name='simple', in_features=3, out_features=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_layer([\n",
    "    [2.0, 2.0, 2.0]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通常我們會希望確定 Model 的 Input Sahpe 之後，再建立 Model 中的 Weight。此時，我們可以透過 Keras Layer (`tf.keras.layers.Layer`) 中的 `build()` Method 做到這件事情。Keras Layer 的 `build()` Method 只會被呼叫一次，而且被呼叫時必須提供 Input Data 的 Shape，因此我們通常會將 Model 的 Weight 的建立放在 `build()` 中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlexibleDense(tf.keras.layers.Layer):\n",
    "\n",
    "    # Keras Layer 的 Constructor 必須包含 '**kwargs'，因為 Keras Layer 還支援很多其他的參數\n",
    "    def __init__(self, out_features, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.out_features = out_features\n",
    "\n",
    "    # 在 Keras Layer 的 build() 中建立 Weight\n",
    "    def build(self, input_shape):\n",
    "        self.w = tf.Variable(tf.random.normal([input_shape[-1], self.out_features]), name='w')\n",
    "        self.b = tf.Variable(tf.zeros([self.out_features]), name='b')\n",
    "\n",
    "    # 在 Keras Layer 的 call() 中定義 Layer 的計算流程 (Computation)\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "flexible_dense = FlexibleDense(out_features=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 因為 Keras Layer 還沒有被 'build' 因此 Weight 還沒有被建立\n",
    "flexible_dense.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[-3.0745847 , -0.79397327, -0.48884273],\n",
       "       [-4.611877  , -1.1909598 , -0.7332642 ]], dtype=float32)>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用 Keras Layer 時 (輸入資料到 Keras Layer 中)，Kera Layer 的 build() 就會被呼叫\n",
    "input_tensor = tf.constant(\n",
    "    [\n",
    "        [2.0, 2.0, 2.0],\n",
    "        [3.0, 3.0, 3.0]\n",
    "    ]\n",
    ")\n",
    "flexible_dense(input_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提醒：因為 Keras Layer 中的 `build()` 只會被呼叫一次，一旦呼叫過後，Keras Layer 中的 Weight 就被建立了，所能接受的 Input Shape 也會固定下來。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'flexible_dense/w:0' shape=(3, 3) dtype=float32, numpy=\n",
       " array([[-0.1036355 ,  0.6960638 , -0.01906842],\n",
       "        [-0.14984168, -1.0727694 ,  0.80770296],\n",
       "        [-1.2838151 , -0.02028105, -1.0330559 ]], dtype=float32)>,\n",
       " <tf.Variable 'flexible_dense/b:0' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flexible_dense.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Matrix size-incompatible: In[0]: [1,4], In[1]: [3,3] [Op:MatMul]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/8h/7gljrm510m7cxjxg8v9rp_k40000gn/T/ipykernel_4233/1319101951.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mflexible_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniforge3/envs/TFEnv/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1005\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1006\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1007\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/8h/7gljrm510m7cxjxg8v9rp_k40000gn/T/ipykernel_4233/2639207331.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# 在 Keras Layer 的 call() 中定義 Layer 的計算流程 (Computation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniforge3/envs/TFEnv/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/TFEnv/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mmatmul\u001b[0;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, name)\u001b[0m\n\u001b[1;32m   3359\u001b[0m             name=name)\n\u001b[1;32m   3360\u001b[0m \u001b[0;31m#--- APPLE_MLCOMPUTE ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m       return gen_math_ops.mat_mul(\n\u001b[0m\u001b[1;32m   3362\u001b[0m           a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n\u001b[1;32m   3363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/TFEnv/lib/python3.8/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmat_mul\u001b[0;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[1;32m   5530\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5531\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5532\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5533\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5534\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/TFEnv/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6860\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6861\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6862\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6863\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/TFEnv/lib/python3.8/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Matrix size-incompatible: In[0]: [1,4], In[1]: [3,3] [Op:MatMul]"
     ]
    }
   ],
   "source": [
    "input_tensor = tf.constant(\n",
    "    [\n",
    "        [2.0, 2.0, 2.0, 2.0]\n",
    "    ]\n",
    ")\n",
    "\n",
    "flexible_dense(input_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 實作 Keras Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們可以透過繼承 `tf.keras.layers.Layer` 建立一個 Keras Layer；同樣的，我們可以透過繼承 `tf.keras.Model` 建立一個 Keras Model。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySequentialModel(tf.keras.Model):\n",
    "    def __init__(self, name=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.dense_1 = FlexibleDense(out_features=3)\n",
    "        self.dense_2 = FlexibleDense(out_features=2)\n",
    "\n",
    "    # 將 Keras Model 的計算流程寫在 call() 中\n",
    "    def call(self, inputs):\n",
    "        x = self.dense_1(inputs)\n",
    "        return self.dense_2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_sequential_model = MySequentialModel(name=\"seq_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[ 12.812953, -12.223006]], dtype=float32)>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor = tf.constant(\n",
    "    [\n",
    "        [2.0, 2.0, 2.0]\n",
    "    ]\n",
    ")\n",
    "\n",
    "my_sequential_model(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'my_sequential_model/flexible_dense_1/w:0' shape=(3, 3) dtype=float32, numpy=\n",
       " array([[-1.3047113 ,  1.8306013 ,  0.41649497],\n",
       "        [ 0.16001828, -1.0010756 , -0.39578095],\n",
       "        [-2.7144563 ,  0.11053286,  1.9898789 ]], dtype=float32)>,\n",
       " <tf.Variable 'my_sequential_model/flexible_dense_1/b:0' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>,\n",
       " <tf.Variable 'my_sequential_model/flexible_dense_2/w:0' shape=(3, 2) dtype=float32, numpy=\n",
       " array([[-1.0462555 ,  1.4136693 ],\n",
       "        [ 1.6629859 ,  0.74568456],\n",
       "        [ 0.40063584, -0.67489016]], dtype=float32)>,\n",
       " <tf.Variable 'my_sequential_model/flexible_dense_2/b:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)>]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看 Keras Model 中的 Variable (Weight)\n",
    "my_sequential_model.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<__main__.FlexibleDense at 0x1264e2b80>,\n",
       " <__main__.FlexibleDense at 0x126c07b20>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看 Keras Model 中的 Submodule\n",
    "my_sequential_model.submodules"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3562e2f13bd7ae4ebcc3409d2c1e5d7ceb582a111492885b652bb97d7858c787"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit ('TFEnv': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
