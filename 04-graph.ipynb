{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TensorFlow Graph 是什麼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在一般情況下，我們可以直接執行 TensorFlow 的 Operation，像是直接執行 Python Statement 那樣，此時我們稱為 Run TensorFlow Eagerly 或是 Eager Execution。\n",
    "\n",
    "除了 Eager Execution 外，TensorFlow 還有另外一種 Execution 方式稱為 Graph Execution。Graph Execution 將所有的 tf.Operation 與 tf.Tensor 包在 tf.Graph 中。\n",
    "\n",
    "tf.Graph 是一種 Data Structure，紀錄所有的 Computation (tf.Operaarion) 與 Data (tf.Tensor)，因此可以不必在 Python 的環境下執行，且比 Eager Execution 擁有更高的效能。\n",
    "\n",
    "下圖為 2 個 Layer 的 Neural Network 的 Graph。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/04/01.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TensorFlow Graph 的優點"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 可攜性高：可以在非 Python 環境下使用 Graph\n",
    "- 速度快：TensorFlow runtime 使用 Grappler optimize TensorFlow Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 透過 tf.function() 建立一個 TensorFlow Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.function() 可以將 Python 中一般的 Function 轉為 TensorFlow Function。TensorFlow Function 的呼叫方式與一般的 Python Function 一樣。當 TensorFlow Function 被呼叫時，會將其所包含的 Operation 轉為一個 TensorFlow Graph，再執行這一個 Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regular_function(x, y, b):\n",
    "    x = tf.matmul(x, y)\n",
    "    x = x + b\n",
    "    return x\n",
    "\n",
    "tensorflow_function = tf.function(regular_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = tf.constant([\n",
    "    [1.0, 2.0]\n",
    "])\n",
    "\n",
    "y1 = tf.constant([\n",
    "    [2.0],\n",
    "    [3.0]\n",
    "])\n",
    "\n",
    "b1 = tf.constant(4.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-28 08:58:31.522581: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-12-28 08:58:31.524385: W tensorflow/core/platform/profile_utils/cpu_utils.cc:126] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[12.]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorflow_function(x1, y1, b1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12.]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regular_function(x1, y1, b1).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了透過 tf.function() 建立 TensorFlow Function，也可以透過 Decorator 建立"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def regular_function(x, y, b):\n",
    "    x = tf.matmul(x, y)\n",
    "    x = x + b\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.eager.def_function.Function at 0x11b886880>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regular_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12.]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regular_function(x1, y1, b1).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert Python logic to TensorFlow Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由上面的 Code 可以發現我們通常會在一般 Function 中寫 TensorFlow Operation 與 Python Logic (if, else, for, while, continue, break)。當我們將一般 Function 轉為 TensorFlow Function 後，再執行這一個 TensorFlow Function，TensorFlow Function 中所有的 Operation 會被建立成一個 TensorFlow Graph。換句話說，TensorFlow Function 中的所有 Code 會被轉為 graph-generating code。\n",
    "\n",
    "TensorFlow Operation 可以比較簡單的被轉為 graph-generating code；Python logic 則需要透過 tf.autograph 將其轉為 graph-generating code。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_relu(x):\n",
    "    if tf.greater(x, 0):\n",
    "        return x\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "tf_simple_relu = tf.function(simple_relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def tf__simple_relu(x):\n",
      "    with ag__.FunctionScope('simple_relu', 'fscope', ag__.ConversionOptions(recursive=True, user_requested=True, optional_features=(), internal_convert_user_code=True)) as fscope:\n",
      "        do_return = False\n",
      "        retval_ = ag__.UndefinedReturnValue()\n",
      "\n",
      "        def get_state():\n",
      "            return (do_return, retval_)\n",
      "\n",
      "        def set_state(vars_):\n",
      "            nonlocal do_return, retval_\n",
      "            (do_return, retval_) = vars_\n",
      "\n",
      "        def if_body():\n",
      "            nonlocal do_return, retval_\n",
      "            try:\n",
      "                do_return = True\n",
      "                retval_ = ag__.ld(x)\n",
      "            except:\n",
      "                do_return = False\n",
      "                raise\n",
      "\n",
      "        def else_body():\n",
      "            nonlocal do_return, retval_\n",
      "            try:\n",
      "                do_return = True\n",
      "                retval_ = 0\n",
      "            except:\n",
      "                do_return = False\n",
      "                raise\n",
      "        ag__.if_stmt(ag__.converted_call(ag__.ld(tf).greater, (ag__.ld(x), 0), None, fscope), if_body, else_body, get_state, set_state, ('do_return', 'retval_'), 2)\n",
      "        return fscope.ret(retval_, do_return)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# graph-generating code\n",
    "print(tf.autograph.to_code(simple_relu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Polymorphism: One Function, Many Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "已知 TensorFlow Function 會建立 TensorFlow Graph，因此傳入 TensorFlow Function 的參數可以視為 TensorFlow Graph 的 Input。Input 的 dtype 與 shape 會紀錄在 Input Signiture。每一個 TensorFlow Graph 都只會有對應的「一個」Input Signiture。\n",
    "\n",
    "當我們傳入不同 dtype 或 shape 的參數到 TensorFlow Function 時，相對應的 TensorFlow Graph 就會被建立。因此一個 TensorFlow Function 其實是 Backed By 很多 TensorFlow Graph。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def my_relu(x):\n",
    "    return tf.maximum(0., x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(5.0, shape=(), dtype=float32)\n",
      "tf.Tensor([1. 0. 3.], shape=(3,), dtype=float32)\n",
      "tf.Tensor([1. 0.], shape=(2,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 3 個 TensorFlow Graph 被建立\n",
    "print(my_relu(tf.constant(5.0)))\n",
    "print(my_relu(tf.constant([1.0, -1.0, 3.0])))\n",
    "print(my_relu(tf.constant([1.0, -1.0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_relu(x)\n",
      "  Args:\n",
      "    x: float32 Tensor, shape=()\n",
      "  Returns:\n",
      "    float32 Tensor, shape=()\n",
      "\n",
      "my_relu(x)\n",
      "  Args:\n",
      "    x: float32 Tensor, shape=(2,)\n",
      "  Returns:\n",
      "    float32 Tensor, shape=(2,)\n",
      "\n",
      "my_relu(x)\n",
      "  Args:\n",
      "    x: float32 Tensor, shape=(3,)\n",
      "  Returns:\n",
      "    float32 Tensor, shape=(3,)\n"
     ]
    }
   ],
   "source": [
    "print(my_relu.pretty_printed_concrete_signatures())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TensorFlow Function: Graph Execution vs Eager Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "照理說，相同的 Code 不管是 Graph Execution 還是 Eager Execution 必須產生相同的結果；然而，在某些情況下會不同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def get_MSE(y_true, y_pred):\n",
    "    sq_diff = tf.pow(y_true - y_pred, 2)\n",
    "    return tf.reduce_mean(sq_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([6 6 3 1 9], shape=(5,), dtype=int32)\n",
      "tf.Tensor([4 0 3 8 7], shape=(5,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "y_true = tf.random.uniform([5], maxval=10, dtype=tf.int32)\n",
    "y_pred = tf.random.uniform([5], maxval=10, dtype=tf.int32)\n",
    "\n",
    "print(y_true)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=18>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_MSE(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow Function 預設是 Graph Execution，手動調成 Eager Execution\n",
    "tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=18>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 發現 Graph Execution 與 Eager Execution 結果相同\n",
    "get_MSE(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必須再手動調回來\n",
    "tf.config.run_functions_eagerly(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在某些情況下，Graph Execution 與 Eager Execution 的結果會不同。例如，在 TensorFlow Function 中加入 print() statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def get_MSE(y_true, y_pred):\n",
    "    sq_diff = tf.pow(y_true - y_pred, 2)\n",
    "    print(\"get_MSE is called !\")\n",
    "    return tf.reduce_mean(sq_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_MSE is called !\n"
     ]
    }
   ],
   "source": [
    "error = get_MSE(y_true, y_pred)\n",
    "error = get_MSE(y_true, y_pred)\n",
    "error = get_MSE(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由上面程式碼發現 print() 只有被執行一次！實際上，當我們呼叫 TensorFlow Function 時，TensorFlow Function 中所有的 Code 會被 Trace 一次，決定哪些 Code 要用來建立 TensorFlow Graph。print() 就是在 Tracing 過程被執行的。\n",
    "\n",
    "因為 print() 不會被放進去 TensorFlow Graph 中，因此當我們呼叫三次 get_MSE() 時，TensorFlow Graph 被執行了三次，卻沒有顯示任何東西。\n",
    "\n",
    "如果將 TensorFlow Function 以 Eager Mode 執行，print() 就每一次都會被執行！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_MSE is called !\n",
      "get_MSE is called !\n",
      "get_MSE is called !\n"
     ]
    }
   ],
   "source": [
    "error = get_MSE(y_true, y_pred)\n",
    "error = get_MSE(y_true, y_pred)\n",
    "error = get_MSE(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seeing the Spped Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow Function 能夠帶來多少 Speed Up 取決於 TensorFlow Function 中的 Operation。TensorFlow Function 常常用來加速 Training Loop。\n",
    "\n",
    "TensorFlow Function 為了帶來 Speed Up 就必須付出代價 —— 建立 TensorFlow Graph 的 Overhead。因此，如果 TensorFlow Function 中的 Operation 很少，反而帶來的 Speed Up 都被「建立」Graph 的 Overhead 給吸收！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.random.uniform(shape=[10, 10], minval=-1, maxval=2, dtype=tf.dtypes.int32)\n",
    "\n",
    "def power(x, y):\n",
    "    result = tf.eye(10, dtype=tf.dtypes.int32)\n",
    "    for _ in range(y):\n",
    "        result = tf.matmul(x, result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0946794999999838"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# eager execution\n",
    "timeit.timeit(lambda: power(x, 100), number=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21099879100006547"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "power = tf.function(power)\n",
    "\n",
    "# graph execution\n",
    "timeit.timeit(lambda: power(x, 100), number=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When is TensorFlow Function tracing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "為了要知道 TensorFlow Function 在什麼時候 Tracing，最簡單即是在 TensorFlow Function 中放入 print()，因為只要 Tracing 發生，print() 就會被執行！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def tracing_function(x):\n",
    "    print(\"Tracing\")\n",
    "    return x*x + tf.constant(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracing\n",
      "#1:  tf.Tensor(27, shape=(), dtype=int32)\n",
      "#2:  tf.Tensor(38, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(\"#1: \", tracing_function(tf.constant(5))) # tracing\n",
    "print(\"#2: \", tracing_function(tf.constant(6))) # no tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracing\n",
      "#1:  tf.Tensor(27, shape=(), dtype=int32)\n",
      "Tracing\n",
      "#2:  tf.Tensor(38, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# 只要 \"Input Signatur\" 改變，就要重新 Tracing 建立 TensorFlow Graph\n",
    "print(\"#1: \", tracing_function(5)) # tracing\n",
    "print(\"#2: \", tracing_function(6)) # tracing"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3562e2f13bd7ae4ebcc3409d2c1e5d7ceb582a111492885b652bb97d7858c787"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit ('TFEnv': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
